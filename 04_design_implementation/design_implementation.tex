\chapter{Analysis Methods}


\section{Methods of Analyzing Alert Data}
\label{sec:anal}
Analyzing the degree of realism for artificially generated alert data is non-trivial. While other fields such as Computer Vision have created well defined metrics such as Inception Score \cite{Salimans2016} or allow for direct human analysis of image quality, no analogue exists for NIDS alerts. Several works have proposed the use of graph based metrics such as comparing nodes and their connectivity for both generated and real network traffic \cite{Siska2010, Iannucci}, or looking at low level parameters such as distributions of packets \cite{Sommers2004, Botta2012}. However, despite these works there is no widely accepted methodology.

It is important to consider the desirable attributes of a \emph{good} metric. A good metric must provide an intuitive, scalable way to summarize what would otherwise be an intractable amount of data to comprehend. Other desirable properties include ways to directly visualize the results of the metric so that trends may be identified visually, able to capture high level dependencies, and tolerance to samples with the value $0$. 

To this end, we propose the usage of several metrics for analyzing NIDS alert data. First, histogram intersection; this metric compares the similarity of two histograms within the same domain by computing the amount of overlap between them. Histogram intersection meets several of the above criterion, as it is naturally bounded between 0 and 1, easily visualized by directly plotting the histograms being compared, and can be extended to accommodate m-many tuples of features. The m-many tuples can be thought of as a joint histogram, where $m$ is the number of unique features considered in the joint. This can be done automatically by iterating over all $M$ choose $m$ \emph{combinations}, where $M$ is the total unique features in the dataset. Mathematically histogram intersection is defined in (\ref{eq:intersection}), where \emph{gt} represents the ground truth data histogram and \emph{gen} represents the generated data histogram, each of which has $N$ samples.

\begin{equation}
\label{eq:intersection}
G = {{\sum_{i=0}^{N} min(gt_i, gen\_i)} \over {max(\sum_{i=0}^{N} gt_i, \sum_{i=0}^{N} gen_i)}}
\end{equation}

Another powerful trait of histogram intersection is that it can be used to reveal dependencies between features within a single alert. This is accomplished by looking at the difference in histogram intersection scores between $n\pm1$ tuples and observing the intersection drop. An intuitive example of this can be thought of as follows; If the intersection for feature $A$ is 0.9 and the intersection for a 2-tuple histogram consisting of features $A$ and $B$ is $0.875$ then it is expected that a dependency exists between $A$ and $B$. It is important to note that this dependency is not inherently bidirectional, as $A$ and $B$ may have varying intersections to begin with. Fig. \ref{fig:metric_graph} illustrates a graph based schema to identify these dependencies visually. 

\begin{figure}[!htbp]
	\centering%
	\includegraphics[width=70mm]{metric_graph}
	\caption{
		Example Feature Graph Highlighting Conditional and Joint Entropy
	}
	\label{fig:metric_graph}
\end{figure}

In order to confirm these dependencies we introduce our second metric; conditional entropy. The conditional entropy of each unique \emph{permutation} may be calculated for each m-tuple of features. Continuing with the previous example this means the conditional entropy of both $A|B$ and $B|A$ are computed. In order to compute a single value that represents the average conditional entropy for all input condition values, the entropy term is computed using (\ref{eq:wce}). This calculation weights the entropy of each possible input combination based off the probability that input $i$ occurs as ${{|w_i|} \over {|w|}}$. ${p_{i|j}}$ represents the probability of the output feature value at index $j$ occurring given the input feature values at index $i$ and must be computed for possible output values $Z$.

\begin{equation}
\label{eq:wce}
\widehat{H}_{Y|X_0, X_1, ..., X_m} = \sum_{i=0}^{N} \bigg({{|w_i|} \over {|w|}} * {\sum_{j=0}^{Z}\Big({p_{i|j} * \log ({1 \over{p_{i|j}}})}\Big)}\bigg)
\end{equation}

In order to further strengthen the argument that there are dependencies between features it is important to consider the overall randomness of the m-tuple joint distribution. To do this (\ref{eq:je}) may be used to compute the joint entropy of the distribution. Using the aforementioned example with features $A$ and $B$, the joint of these variables is denoted $A,B$.

\begin{equation}
\label{eq:je}
{H}_{X_m} = -\sum_{x_m} p(x_0, x_1,...,x_m)  * \log \big({{p(x_0,x_1,...,x_m)}}\big)
\end{equation}

Given that natural logarithms are used for the calculation in (\ref{eq:wce}) and (\ref{eq:je}), the resulting value is given in the natural unit of information (nats), a log base e equivalent to bits. Given that the distribution of m-tuple feature histograms is a discrete distribution with finite support the upper bound of entropy is given by the uniform distribution $mathbb{U}$. Note that the cardinality of $mathbb{U}$ varies to match the number of unique values in the conditional or joint probability being normalized. Using this quantity (\ref{eq:wce}) can be normalized as shown in (\ref{eq:norm_wce}) and (\ref{eq:norm_je}). This has the benefit of naturally bounding the entropies between 0 and 1, similar to the intersection score defined in (\ref{eq:intersection}).


\begin{equation}
\label{eq:norm_wce}
\overline{H}_{Y|X_0, X_1, ..., X_m} = {{\widehat{H}_{Y|X_0, X_1, ..., X_m}} \over{ H(\mathbb{U})}}
\end{equation}


\begin{equation}
\label{eq:norm_je}
\overline{H}_{X_m} = {{{H}_{X_m}} \over{ H(\mathbb{U})}}
\end{equation}

If the drop in intersections is indeed correlated to feature dependency, then the conditional entropy should be directly proportional to this drop. Additionally, the benefit of capturing feature dependencies is apparent by comparing to the joint entropy without any conditioning information. Note that in Fig. \ref{I haven't made this yet} the edges corresponding to conditional and joint entropies are labeled using the notation given in the examples above.

One drawback of using the intersection of histograms is that the metric does not perform well when the ground truth distribution evaluated is highly deterministic. The data generating model can learn to output that value in a purely deterministic manner and receive an intersection score equivalent to the probability of that given value occurring in the ground truth set. In order to avoid this issue a metric which penalizes failure to encompass the entire domain of output values. 

%TODO: Include equations for completeness

Kullback Leibler (KL) divergence was considered as a candidate, however it does not have the property of zero tolerance and is asymmetric. However, the Jensen Shannon (JS) divergence is both zero tolerant and symmetric. This metric has the downside of having no upper bound and by extension is not as intuitive as the previously proposed metrics with firm bounds. This metric can be used as a drop in replacement for the histogram intersection and still be used in conjunction with the weighted conditional and joint entropies.

\section{Other Useful Ways to Analyze Alerts}

The purpose of this section is to review other methods of analyzing artificially generated alerts. The methods presented in this section are comprised of subsidiary steps for the computation of the previously presented metrics and or provide useful information in understanding what is driving model behavior. 

\subsection{Using Conditional Probability Tables to Evaluate Generated Alerts}

In the aforementioned (\ref{eq:wce}) weighted conditional entropy is computed to provide a singular score representing conditional randomness in m-tuple histograms. An intermediary step in this computation involves the creation of conditional probability tables which show all possible input conditioning values and their impact on output value probability. It was found through experimentation that directly outputting these tables and applying highlighting to reveal sparsity and determinism is an effective means to evaluate specific feature value relationships. This method has the drawback of becoming intractable as the number of tables generated is equal to the number of unique feature permutations. A sample table is given in Section \ref{sec:rna} to illustrate this point. 

\subsection{Measuring Output Mode Capture}

One important attribute of artificially generated data is the number of output modes that the model manages to capture. Traditionally, GANs have suffered from output mode collapse, where outputs that have a low probability of occurring in the ground truth do not occur ever in the model's output. This can be thought of as a false negative for the model. Additionally, if the GAN has not been trained sufficiently then there is the potential for it to generate noisy samples which never occurred in the ground truth dataset. This can be thought of as a false positive. 

Generative a table for both the false negatives and false positives of the model can be useful in identifying faults within the model while hyper parameter tuning. Ideally, both of these values should be driven towards zero if the model is performing well. Note that this does not indicate a perfect model however, as the probability distribution for output modes may not reflect those of the ground truth distribution. 
