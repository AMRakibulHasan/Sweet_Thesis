\chapter{Introduction}

\section{Network Intrusion Detection Systems}
A variety of packet capture tools exist to enable individuals and corporations alike to monitor traffic on their networks. Several tools take this logging a step further and allow for real time network intrusion detection. Network intrusion detection systems (NIDS) are a rule based system which allows for automated flagging of potentially malignant packet traffic through the aggregation of temporally contiguous packets;  these "alerts" typically consist of the believed attack signature, a category that the attack falls under, target machine, timestamp, and more. These systems are employed to flag potentially malicious traffic, note long term patterns in traffic, and provide system administration with a trail of alerts to analyze after a cyber-attack has taken place. 

Though these tools provide network operators with a wealth of data to analyze they suffer from a fundamental issue. The data only exists after a cyber attack has already taken place, and therefore only allows for reactionary defense techniques. This has lead researchers to see if they can use this data, as well as simulations and extrapolation, to further understand network vulnerabilities in the pursuit of proactive cyber security. 

\section{Challenges of Cyber Alert Data}
Cyber Alert data suffers from two primary challenges; There is a lack of malicious traffic data and the data that does exist is imbalanced, non-homogeneous, and unlabeled. 

Access to malicious NIDS alert data has been a long standing problem for researchers in the domain of Cyber Security. As an alternative probablistic models, such as attack graphs \cite{Qin2004, Wang2006, Noel2009}, have been proposed and updated to try and create realistic attack data for varying network architectures in an automated fashion. These models provide insight into potential attack paths within a network, the probability a given path will be used, and what vulnerabilities within the network allow for these paths to exist. Other works have defined methods to model these attack graphs as Markov Chains \cite{Li2017}, while others have employed statistical graph models \cite{Du2014} and Variable Length Markov Models \cite{Fava2008} in attempts to better understand potential vulnerabilities. Despite the effectiveness of these models however they lack any means to consider historical attack data or consider real network alerts. 

Of the datasets which do exist, common issues include small data set size, high imbalance between malignant and benign alerts, redundant samples, and intricate interactions leading to misleading labels. One example of this is the KDD Cup '99 \cite{kdd-cup} dataset. This dataset was prepared by Stolfo \etal \cite{Stolfo} based off data captured in DARPA'98 IDS evaluation program \cite{Lippmann}. It consists of samples from 7 weeks of network traffic collected via TCPdump that was labeled with one of 5 labels {normal, denial of service, user to root attack, remote to local attack, or probing attack}. Recently, it been used for multiple studies involving cyber attack classification and prediction through the use of recurrent neural networks (RNNs) \cite{Kim, Staudemeyer}. However this dataset contains several pathological issues such as synthetic background traffic, underlying issues with TCPdump under intense load, and a lack of precise definitions for what constitutes an attack, as highlighted by Tavallaee \etal \cite{Tavallaee} and McHugh \cite{McHugh}.

Other publicly available datasets struggle with a low signal to noise ratio. Two examples of this are the Multi Source Cyber Event Dataset published by Los Alamos National Laboratory \cite{akent-2015-enterprise-data} and the DeepSecurity Dataset released by Faber and Malloy \cite{Faber2018}. The Multi Source Cyber Event Dataset contains 4.8 KB of textual information pertaining to malicious events. The magnitude of these events pales in comparison to the overall scale of the dataset, which is encompassed in 12.2 GB of textual data. DeepSecurity faces a similar issue with a low percentage of their 600,000 network events being representative of malicious network traffic. The authors note that the availability of quality labeled data and a low signal to noise ratio for malicious activity are both outstanding issues with their studies \cite{Faber2018}. 

Though the distribution of malicious events in these datasets may be representative of real world cyber alert data it creates many challenges for network defense. These challenges include the potential obfuscation of attack behavior, difficulty isolating malicious alerts interspersed throughout a stream of non-malicious alerts, and no ground truth labels. With no commonly accepted means to artificially generate additional malicious alert data, these challenges persist in the field of Cyber Security. 

\section{Generative Adversarial Networks}

A Generative Adversarial Network (GAN) is a class of neural network where two neural networks are pitted against each other. One network, the generator, attempts to create samples which seem to belong to a ground truth dataset. The other network, the discriminator, takes inputs from the ground truth dataset as well as the generator and flags samples as either real or fake. This structure minimizes the generator loss each time the generator successfully creates a sample that tricks the discriminator into marking the sample as real. Conversely, the discriminator loss is minimized when all samples from the ground truth set are marked as real and all samples created by the generator are marked as fake.

GANs have achieved state of the art results in generating data with respect to images \cite {Karras2018, Zhu2017, Ledig2016}, text \cite{Su2018}, and sound \cite{Dong2018, Gao2018}.  Additionally, they have also been shown to perform well at more complex tasks such as scene to scene translation in images \cite{Zhu2017, Choi2017} and stylized image generation \cite{Karras2018}. These architectures allow GANs to serve as a powerful tool to artificially expand datasets. Additionally, high fidelity data generation requires the generator to learn key dependencies between features within each generated sample. A means to reveal and analyze these dependencies would be a powerful tool for analyzing critical features within the dataset.

Despite these successes, GANs do have several shortcomings. They are noted for requiring large numbers of samples per class and are typically trained across very large datasets for many epochs. The loss functions do not represent the quality of the data, as both the generator and discriminator are continually learning; as one network becomes better it's loss may drop, only to rise back up in a few batches when the other network learns something new as well. This lack of convergence makes training and hyperparameter tuning even more important than in traditional Deep Learning models. Allowing one model to overpower the other starves the system from having any useful gradient feedback. Finally, output mode dropping may occur, as the generator may not receive sufficient gradient feedback to encourage full exploration of the dataset.

\section{Problem Statement}

In order to address the lack of malicious NIDS alert data for cyber attack studies, we explore the usage of GANs as a means to recreate and expand existing alert data. The application of GANs is challenged by potential for output mode collapse and failure of the network to learn realistic output distributions for each feature. Generalized preprocessing techniques are defined to prepare NIDS alert data for usage with GANs while also making model inputs and outputs more intuitive to analysis. 

Since there is no commonly accepted metric to score the fidelity of synthetically generated alerts, several desirable attributes for alert fidelity scoring are defined. Subsequently, histogram intersection and Jensen Shannon divergence are proposed as metrics which meet these criteria and are used to evaluate synthetically generated alerts from a GAN. Advantages and disadvantages of each metric are reviewed. 

Furthermore, conditional entropy and joint entropy are suggested as means to measure the efficacy of the GAN on learning the intricate feature interactions within an alert. Conditional probability tables are also employed to further understand the degree to which GANs learn feature dependencies. These methods provide insight into the critical features of alert data even when applied outside the context of analyzing synthetically generated alerts. Additionally, they provide detailed contextual information by allowing researchers to directly analyze feature-value relationships in alert data. 

Finally, mutual information maximization is proposed as a means to regularize the generators output. This addition encourages further exploration of the ground truth dataset and reduces output mode collapse.  

% With each of the above proposed metrics there are unique insights and drawbacks. Histogram representation of the dataset allows the data to be analyzed wholelistically but is subject to bias when outputs are dominated by a single feature value. The scores provided by these metrics may also fail to account for the complexity of the dataset or fail to represent useful data. For example, the intersection of histograms between alert signatures would not be a full representation of cyber events, as alert signature alone does not provide enough contextual information to be useful. However, when coupled with other features, such as destination port or latent network information such as the target machine's purpose (e.g. HTTP Server) salient attack information may be inferred. This can lead to insights regarding the ability of the network to learn the behavior of the attackers, rather than replicate their actions identically. Comparing conditional entropy tables for the ground truth and generated dataset can help to identify critical relationships between features. That is to say, some feature values may only be learned by the generator because of their relationship to another feature's value.

% To identify the fidelity a GAN recreates cyber alerts with, methods evaluating said fidelity must be defined. One proposed metric is to use the intersection of histograms for each feature, and each possible combination of features, between the real and generated dataset. This metric has the benefit of being intuitive to visualize and analyze for low dimensional representations of the data. Additionally, the Jensen Shannon Divergence may also be used to compare alerts from the ground truth and generated set; this metric has the benefit of a nonlinear penalty for failing to capture the true probability of specific outputs. Each of these metrics have a rich history for comparing discrete feature histograms. Finally, Joint and Conditional Entropy between varying alert features are shown to be able to hightlight the difficulty of generating a given feature, as well as critical feature relationships in each distribution. These values also reinforce the power of GANs, as they may be used to show that these feature dependencies are indeed captured by the model and important to successfully generating features which would not occur often if considered in isolation. 


