% background.tex
%
% Author       : James Mnatzaganian
% Contact      : http://techtorials.me
% Date Created : 08/27/15
%
% Description  : Background chapter used by "thesis.tex".
%
% Copyright (c) 2015 James Mnatzaganian

\chapter{Conclusions and Future Work}

\section{Conclusion}

\section{Future Work}

\subsection{Multi-Alert Generation and Analysis}

\subsection{Improving Generation through Reinforcement Learning}

Methods of establishing generator loss in a generative adversarial networks fail to capture discrete features which have interdependencies. This work proposes a way to model generator loss as a game and apply a reinforcement learning inspired solution to it. 

Consider the generator to be an agent, whose action space is comprised of all the potential output features it may generate. Then, consider the following  hierarchical reward scheme: For each feature that is output correctly reward the generator with $2^1$ 'points'. For each pair of features reward the generator with $2^2$ 'points'. And so on and so forth where the maximal reward is $2^n$ 'points'. The n-many tuple that is correct can be considered the order of the output (e.g.) The above examples would be first order, second order, and finally n-th order. This can be established as a sum of the correct combinations as follows:

{
	\centering
	Given A is the action space consisting of all unique features in the dataset\\
	and s is a sample action generated by the neural network drawn from A\\
}

\begin{equation}
L = \sum_{k=1}^{n}\sum_{s_{{(^n_k)}} \in A_{(^n_k)}} 2^k
\end{equation}

This formulation has the following benefits:
\begin{enumerate}
	\item Encourages a generator which captures inter-feature dependencies during training
	\item Can be trained without the discriminator network, lowering the potential intractability of complex networks
	\begin{itemize}
		\item Note that this comes at the expense of a loss funciton with O(n!) runtime, arguably intractible for actions with a high rank
	\end{itemize}
	\item Can be modified to target only a specific combination of features
	\item Increases reward size non-linearly to as to acccount for the significant difficulty increase in getting n-many tuples correct.
\end{enumerate}

Another benefit of formulating loss in this manner is that it allows for easy expandability. Two examples are as follows: 
\begin{enumerate}
	\item The loss may be modified to provide a reward proportional to the probability of the selected character being generated. 
	\item The loss may be modeled as a n-th order Markov Source to capture temporal structure within the data. 
\end{enumerate}

The probability mass function for cyberattack data does not follow any conventional distribution (Normal, Poisson, Uniform, etc), nor does it account for all possible features in the 'Alert Space' [Explain this]. In order to account for this in the loss function each feature predicted that is in the Alert Space would have a weighted reward value by the probability of that feature value occuring as shown below:

{
	\centering
	Let F be a subset of features from the Alert Space A\\
	$F_v$ represents a specific value for the Feature f.\\
}
\begin{gather}
p_{v_n} = {|F_{v_n}| \over |F_n|}\\
L = \sum_{k=1}^{n}\sum_{s_{{(^n_k)}} \in A_{(^n_k)}} (2^k)p(v_1)
\end{gather}


This can be further expanded to account for the inpact of other features on the target prediction feature. Conceptually, if trying to compute the loss of feature A given feature B as a prior, the conditional probability of A may be computed. This would further encourage inter-feature dependency. 

An example of this would be computing the probability of a specific value for Destination Port given that the value for IP is already determined to be 129.21.154.3. On the other hand, it is still important to value results which occur in the global action space even if they don't occur with the specific IP value given; the intuition behind this is that the generated should also be encouraged to explore the action space, not just exploit it. To do this the joint probability should be used with the naive probability proposed above. 

\begin{gather}
L = \sum_{k=1}^{n-1}\sum_{s_{{(^n_k)}} \in A_{(^n_k)}} (2^k)p(v_1) + \sum_{k=1}^{n-1}\sum_{s_{{(^n_k)}} \in A_{(^n_k)}} (2^k)p(v_1|v_2)
\end{gather}

Such a conditional probability model can also be used to model temporal relations within the data. This is accomplished by using the previous alerts feature value as a prior for the current generated feature. This in turn can be extended to an n-th order markov model where n alert features act as priors for the current feature being generated. This may be represented as shown below:

\begin{gather}
L =  \sum_{k=1}^{n-1}\sum_{s_{{(^n_k)}} \in A_{(^n_k)}} (2^k)p(v_1|v_2, v_3, ..., v_n))
\end{gather}

